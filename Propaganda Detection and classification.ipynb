{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ljOyoHLKHVPI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\r25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\r25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import string\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Read training data from CSV file\n",
    "train_df = pd.read_csv(\"propaganda_train.tsv\",delimiter='\\t', names=['label', 'sentence'])\n",
    "# Read validation data from CSV file\n",
    "val_df = pd.read_csv(\"propaganda_val.tsv\", delimiter='\\t', names=['label', 'sentence'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIt8tpLVJNRi",
    "outputId": "732a3c64-3f0a-4b67-d31b-96aa06fb1932",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       label  \\\n",
      "0                      label   \n",
      "1             not_propaganda   \n",
      "2  causal_oversimplification   \n",
      "3   appeal_to_fear_prejudice   \n",
      "4             not_propaganda   \n",
      "\n",
      "                                            sentence  \n",
      "0                                  tagged_in_context  \n",
      "1  On average, between 300 and 600 infections are...  \n",
      "2  Mostly because <BOS> the country would not las...  \n",
      "3  Lyndon Johnson <BOS> gets Earl Warren and Sen....  \n",
      "4           <BOS> You <EOS> may opt out at anytime.   \n"
     ]
    }
   ],
   "source": [
    "print(val_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    special_tokens = ['<BOS>', '<EOS>', 'eos', 'bos', '<eos>', '<bos>']\n",
    "    for token in special_tokens:\n",
    "        text = text.replace(token, '')\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    remove_tokens = set(stopwords.words('english') + list(string.punctuation))\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [word for word in tokens if word not in remove_tokens]\n",
    "    words = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "train_df['sentence'] = train_df['sentence'].apply(preprocess_text)\n",
    "val_df['sentence'] = val_df['sentence'].apply(preprocess_text)\n",
    "\n",
    "# Remove erroneous 'label' entries\n",
    "train_df = train_df[train_df['label'] != 'label']\n",
    "val_df = val_df[val_df['label'] != 'label']\n",
    "\n",
    "# Update labels for binary classification: 1 for any propaganda technique, 0 for 'not_propaganda'\n",
    "train_df['binary_label'] = (train_df['label'] != 'not_propaganda').astype(int)\n",
    "val_df['binary_label'] = (val_df['label'] != 'not_propaganda').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'sentence', 'binary_label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       label  \\\n",
      "1             not_propaganda   \n",
      "2  causal_oversimplification   \n",
      "3   appeal_to_fear_prejudice   \n",
      "4             not_propaganda   \n",
      "5                 repetition   \n",
      "\n",
      "                                            sentence  binary_label  \n",
      "1  averag 300 600 infect record everi year among ...             0  \n",
      "2  mostli countri would last long without outsid ...             1  \n",
      "3  lyndon johnson get earl warren sen. richard ru...             1  \n",
      "4                                     may opt anytim             0  \n",
      "5  must exact directli order vilifi humili islam ...             1  \n"
     ]
    }
   ],
   "source": [
    "print(val_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            label                                           sentence  \\\n",
      "1  not_propaganda                                            confirm   \n",
      "2  not_propaganda    declassif effort ’ make thing wors presid trump   \n",
      "3     flag_waving  obama administr misl american peopl congress d...   \n",
      "4  not_propaganda  “ look like ’ captur demis dark vortex ’ diffe...   \n",
      "5  not_propaganda                               locat westervil ohio   \n",
      "\n",
      "   binary_label  \n",
      "1             0  \n",
      "2             0  \n",
      "3             1  \n",
      "4             0  \n",
      "5             0  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'sentence', 'binary_label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTI-W8ElI_x2",
    "outputId": "750ba141-5a30-4e3c-a379-bf2ea038a217",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8910521955260977\n",
      "Testing Accuracy: 0.6827586206896552\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(train_df['sentence'])\n",
    "X_test_vec = vectorizer.transform(val_df['sentence'])\n",
    "\n",
    "#Hyperparameter Tuning: MultinomialNB with Laplace smoothing\n",
    "clf = MultinomialNB(alpha=1.0)  # Laplace smoothing (alpha=1.0)\n",
    "clf.fit(X_train_vec, train_df['binary_label'])\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = clf.predict(X_train_vec)\n",
    "# Calculate training accuracy\n",
    "train_accuracy = accuracy_score(train_df['binary_label'], y_train_pred)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = clf.predict(X_test_vec)\n",
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(val_df['binary_label'], y_test_pred)\n",
    "\n",
    "# Print training and testing accuracies\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9MtQ57ajwgO",
    "outputId": "9abc2401-3d6e-4292-d41c-79c7a8dc70f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier:\n",
      "Training Accuracy: 0.7415\n",
      "Testing Accuracy: 0.5655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub-0.23.0-py3.8.egg\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU: NVIDIA RTX A4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 151/151 [01:46<00:00,  1.42it/s]\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 1:\n",
      "  Training Loss: 1.7477, Accuracy: 0.4743\n",
      "  Validation Loss: 1.5958, Accuracy: 0.5190\n",
      "BERT Classifier:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.00      0.00      0.00        43\n",
      "causal_oversimplification       0.00      0.00      0.00        31\n",
      "                    doubt       0.00      0.00      0.00        38\n",
      "exaggeration,minimisation       0.00      0.00      0.00        28\n",
      "              flag_waving       0.00      0.00      0.00        39\n",
      "          loaded_language       0.00      0.00      0.00        37\n",
      "    name_calling,labeling       0.00      0.00      0.00        31\n",
      "           not_propaganda       0.52      1.00      0.68       301\n",
      "               repetition       0.00      0.00      0.00        32\n",
      "\n",
      "                 accuracy                           0.52       580\n",
      "                macro avg       0.06      0.11      0.08       580\n",
      "             weighted avg       0.27      0.52      0.35       580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 151/151 [01:46<00:00,  1.41it/s]\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 2:\n",
      "  Training Loss: 1.5701, Accuracy: 0.4640\n",
      "  Validation Loss: 1.5206, Accuracy: 0.5414\n",
      "BERT Classifier:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.50      0.09      0.16        43\n",
      "causal_oversimplification       0.12      0.13      0.12        31\n",
      "                    doubt       1.00      0.03      0.05        38\n",
      "exaggeration,minimisation       0.00      0.00      0.00        28\n",
      "              flag_waving       0.34      0.46      0.39        39\n",
      "          loaded_language       0.00      0.00      0.00        37\n",
      "    name_calling,labeling       0.00      0.00      0.00        31\n",
      "           not_propaganda       0.59      0.95      0.73       301\n",
      "               repetition       0.00      0.00      0.00        32\n",
      "\n",
      "                 accuracy                           0.54       580\n",
      "                macro avg       0.28      0.18      0.16       580\n",
      "             weighted avg       0.44      0.54      0.43       580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 151/151 [01:48<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 3:\n",
      "  Training Loss: 1.3609, Accuracy: 0.3691\n",
      "  Validation Loss: 1.4863, Accuracy: 0.5500\n",
      "BERT Classifier:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.34      0.37      0.36        43\n",
      "causal_oversimplification       0.00      0.00      0.00        31\n",
      "                    doubt       0.00      0.00      0.00        38\n",
      "exaggeration,minimisation       1.00      0.04      0.07        28\n",
      "              flag_waving       0.57      0.54      0.55        39\n",
      "          loaded_language       0.20      0.03      0.05        37\n",
      "    name_calling,labeling       0.13      0.29      0.18        31\n",
      "           not_propaganda       0.66      0.89      0.76       301\n",
      "               repetition       0.20      0.06      0.10        32\n",
      "\n",
      "                 accuracy                           0.55       580\n",
      "                macro avg       0.34      0.25      0.23       580\n",
      "             weighted avg       0.49      0.55      0.48       580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 151/151 [01:48<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 4:\n",
      "  Training Loss: 1.0949, Accuracy: 0.3007\n",
      "  Validation Loss: 1.5844, Accuracy: 0.5397\n",
      "BERT Classifier:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.25      0.49      0.33        43\n",
      "causal_oversimplification       0.22      0.16      0.19        31\n",
      "                    doubt       0.67      0.05      0.10        38\n",
      "exaggeration,minimisation       0.00      0.00      0.00        28\n",
      "              flag_waving       0.47      0.59      0.52        39\n",
      "          loaded_language       0.18      0.11      0.14        37\n",
      "    name_calling,labeling       0.13      0.06      0.09        31\n",
      "           not_propaganda       0.67      0.85      0.75       301\n",
      "               repetition       0.00      0.00      0.00        32\n",
      "\n",
      "                 accuracy                           0.54       580\n",
      "                macro avg       0.29      0.26      0.23       580\n",
      "             weighted avg       0.47      0.54      0.48       580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 151/151 [01:47<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 5:\n",
      "  Training Loss: 0.8884, Accuracy: 0.2962\n",
      "  Validation Loss: 1.7038, Accuracy: 0.5448\n",
      "BERT Classifier:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.36      0.30      0.33        43\n",
      "causal_oversimplification       0.19      0.45      0.27        31\n",
      "                    doubt       0.50      0.05      0.10        38\n",
      "exaggeration,minimisation       0.23      0.11      0.15        28\n",
      "              flag_waving       0.53      0.51      0.52        39\n",
      "          loaded_language       0.25      0.05      0.09        37\n",
      "    name_calling,labeling       0.16      0.10      0.12        31\n",
      "           not_propaganda       0.68      0.85      0.75       301\n",
      "               repetition       0.29      0.12      0.17        32\n",
      "\n",
      "                 accuracy                           0.54       580\n",
      "                macro avg       0.35      0.28      0.28       580\n",
      "             weighted avg       0.51      0.54      0.50       580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 151/151 [01:48<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 6:\n",
      "  Training Loss: 0.7141, Accuracy: 0.2850\n",
      "  Validation Loss: 1.7764, Accuracy: 0.5259\n",
      "BERT Classifier:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.30      0.26      0.28        43\n",
      "causal_oversimplification       0.15      0.26      0.19        31\n",
      "                    doubt       0.17      0.13      0.15        38\n",
      "exaggeration,minimisation       0.33      0.14      0.20        28\n",
      "              flag_waving       0.62      0.54      0.58        39\n",
      "          loaded_language       0.18      0.05      0.08        37\n",
      "    name_calling,labeling       0.27      0.13      0.17        31\n",
      "           not_propaganda       0.68      0.81      0.74       301\n",
      "               repetition       0.22      0.22      0.22        32\n",
      "\n",
      "                 accuracy                           0.53       580\n",
      "                macro avg       0.32      0.28      0.29       580\n",
      "             weighted avg       0.49      0.53      0.50       580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Traditional Machine Learning with TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df['sentence'])\n",
    "X_val_tfidf = vectorizer.transform(val_df['sentence'])\n",
    "\n",
    "y_train = train_df['label'].tolist()\n",
    "y_val = val_df['label'].tolist()\n",
    "\n",
    "# Encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "# Train SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "# Evaluate SVM classifier\n",
    "svm_train_predictions = svm_classifier.predict(X_train_tfidf)\n",
    "svm_val_predictions = svm_classifier.predict(X_val_tfidf)\n",
    "\n",
    "svm_train_accuracy = accuracy_score(y_train_encoded, svm_train_predictions)\n",
    "svm_val_accuracy = accuracy_score(y_val_encoded, svm_val_predictions)\n",
    "\n",
    "print(\"SVM Classifier:\")\n",
    "print(f\"Training Accuracy: {svm_train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {svm_val_accuracy:.4f}\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define BERT DataLoader for training and validation\n",
    "batch_size = 16\n",
    "\n",
    "train_encodings = tokenizer(list(train_df['sentence']), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(val_df['sentence']), truncation=True, padding=True)\n",
    "\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "train_labels = torch.tensor(y_train_encoded, dtype=torch.long)  # Use encoded labels\n",
    "\n",
    "val_inputs = torch.tensor(val_encodings['input_ids'])\n",
    "val_masks = torch.tensor(val_encodings['attention_mask'])\n",
    "val_labels = torch.tensor(y_val_encoded, dtype=torch.long)  # Use encoded labels\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    # CUDA is available, print the CUDA device\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    # CUDA is not available, use CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(train_df['label'].unique()))\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Train BERT model\n",
    "epochs = 6\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_predictions = []\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_predictions.extend(torch.argmax(outputs.logits, dim=1).cpu().detach().numpy())\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_predictions, val_true_labels = [], []\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "            val_predictions.extend(torch.argmax(outputs.logits, dim=1).cpu().detach().numpy())\n",
    "            val_true_labels.extend(inputs[\"labels\"].cpu().detach().numpy())\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    train_accuracy = accuracy_score(y_train_encoded, train_predictions)\n",
    "    val_accuracy = accuracy_score(y_val_encoded, val_predictions)\n",
    "\n",
    "    print(f\"BERT Classifier:\")\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(f\"  Training Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Evaluate BERT classifier\n",
    "    print(\"BERT Classifier:\")\n",
    "    print(classification_report(y_val_encoded, val_predictions, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGvb3rzEzS84",
    "outputId": "16e06fb9-9875-465c-f909-a8c9107df55e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.46      0.14      0.21        43\n",
      "causal_oversimplification       0.20      0.03      0.06        31\n",
      "                    doubt       0.40      0.11      0.17        38\n",
      "exaggeration,minimisation       0.50      0.11      0.18        28\n",
      "              flag_waving       0.76      0.41      0.53        39\n",
      "          loaded_language       0.00      0.00      0.00        37\n",
      "    name_calling,labeling       0.20      0.03      0.06        31\n",
      "           not_propaganda       0.57      0.97      0.72       301\n",
      "               repetition       0.83      0.16      0.26        32\n",
      "\n",
      "                 accuracy                           0.57       580\n",
      "                macro avg       0.44      0.22      0.24       580\n",
      "             weighted avg       0.50      0.57      0.46       580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate SVM classifier\n",
    "print(\"SVM Classifier:\")\n",
    "print(classification_report(y_val_encoded, svm_val_predictions, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aic5PyVczZJ4",
    "outputId": "3b74994c-f6af-462c-fe01-5ac4741a46ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " appeal_to_fear_prejudice       0.30      0.26      0.28        43\n",
      "causal_oversimplification       0.15      0.26      0.19        31\n",
      "                    doubt       0.17      0.13      0.15        38\n",
      "exaggeration,minimisation       0.33      0.14      0.20        28\n",
      "              flag_waving       0.62      0.54      0.58        39\n",
      "          loaded_language       0.18      0.05      0.08        37\n",
      "    name_calling,labeling       0.27      0.13      0.17        31\n",
      "           not_propaganda       0.68      0.81      0.74       301\n",
      "               repetition       0.22      0.22      0.22        32\n",
      "\n",
      "                 accuracy                           0.53       580\n",
      "                macro avg       0.32      0.28      0.29       580\n",
      "             weighted avg       0.49      0.53      0.50       580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate BERT classifier\n",
    "print(\"BERT Classifier:\")\n",
    "print(classification_report(y_val_encoded, val_predictions, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Classifier:\n",
      "Training Accuracy: 0.8659\n",
      "Testing Accuracy: 0.3978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub-0.23.0-py3.8.egg\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 77/77 [00:50<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 1:\n",
      "  Training Loss: 1.9719, Accuracy: 0.1325\n",
      "  Validation Loss: 1.8509, Accuracy: 0.2688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 77/77 [00:49<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 2:\n",
      "  Training Loss: 1.7341, Accuracy: 0.1308\n",
      "  Validation Loss: 1.7466, Accuracy: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 77/77 [00:49<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 3:\n",
      "  Training Loss: 1.4847, Accuracy: 0.1259\n",
      "  Validation Loss: 1.6573, Accuracy: 0.3978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 77/77 [00:49<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 4:\n",
      "  Training Loss: 1.1980, Accuracy: 0.1235\n",
      "  Validation Loss: 1.6331, Accuracy: 0.4373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 77/77 [00:49<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 5:\n",
      "  Training Loss: 0.9172, Accuracy: 0.1259\n",
      "  Validation Loss: 1.6346, Accuracy: 0.4480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 77/77 [00:50<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 6:\n",
      "  Training Loss: 0.6112, Accuracy: 0.1496\n",
      "  Validation Loss: 1.7150, Accuracy: 0.4552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 77/77 [00:50<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 7:\n",
      "  Training Loss: 0.4493, Accuracy: 0.1300\n",
      "  Validation Loss: 1.7592, Accuracy: 0.4803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 77/77 [00:49<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 8:\n",
      "  Training Loss: 0.2995, Accuracy: 0.1382\n",
      "  Validation Loss: 1.9215, Accuracy: 0.4695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 77/77 [00:49<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 9:\n",
      "  Training Loss: 0.2279, Accuracy: 0.1161\n",
      "  Validation Loss: 1.9740, Accuracy: 0.4767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 77/77 [00:49<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 10:\n",
      "  Training Loss: 0.1686, Accuracy: 0.1169\n",
      "  Validation Loss: 2.1411, Accuracy: 0.4659\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(\"propaganda_train.tsv\", delimiter='\\t', names=['label', 'sentence'])\n",
    "val_df = pd.read_csv(\"propaganda_val.tsv\", delimiter='\\t', names=['label', 'sentence'])\n",
    "\n",
    "# Define preprocessing functions\n",
    "def extract_text_between_bos_eos(text):\n",
    "    pattern = re.compile(r'<BOS>(.*?)<EOS>')\n",
    "    matches = pattern.findall(text)\n",
    "    return ' '.join(matches)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower().replace('<bos>', '').replace('<eos>', '')\n",
    "    tokens = word_tokenize(text)\n",
    "    remove_tokens = set(stopwords.words('english') + list(string.punctuation))\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in remove_tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['tagged_in_context'] = train_df['sentence'].apply(extract_text_between_bos_eos)\n",
    "val_df['tagged_in_context'] = val_df['sentence'].apply(extract_text_between_bos_eos)\n",
    "\n",
    "train_df['tagged_in_context'] = train_df['tagged_in_context'].apply(preprocess_text)\n",
    "val_df['tagged_in_context'] = val_df['tagged_in_context'].apply(preprocess_text)\n",
    "\n",
    "# Remove erroneous 'label' entries\n",
    "train_df = train_df[train_df['label'] != 'label']\n",
    "val_df = val_df[val_df['label'] != 'label']\n",
    "\n",
    "# Filter out 'not_propaganda' entries\n",
    "train_df = train_df[train_df['label'] != 'not_propaganda']\n",
    "val_df = val_df[val_df['label'] != 'not_propaganda']\n",
    "\n",
    "# Traditional Machine Learning with CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df['tagged_in_context'])\n",
    "X_val_tfidf = vectorizer.transform(val_df['tagged_in_context'])\n",
    "\n",
    "y_train = train_df['label'].tolist()\n",
    "y_val = val_df['label'].tolist()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "# Train Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "# Evaluate Multinomial Naive Bayes classifier\n",
    "nb_train_predictions = nb_classifier.predict(X_train_tfidf)\n",
    "nb_val_predictions = nb_classifier.predict(X_val_tfidf)\n",
    "\n",
    "nb_train_accuracy = accuracy_score(y_train_encoded, nb_train_predictions)\n",
    "nb_val_accuracy = accuracy_score(y_val_encoded, nb_val_predictions)\n",
    "\n",
    "print(\"MultinomialNB Classifier:\")\n",
    "print(f\"Training Accuracy: {nb_train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {nb_val_accuracy:.4f}\")\n",
    "\n",
    "# Fine-tune BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(train_df['label'].unique()))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "batch_size = 16\n",
    "train_encodings = tokenizer(list(train_df['tagged_in_context']), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(val_df['tagged_in_context']), truncation=True, padding=True)\n",
    "\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "train_labels = torch.tensor(y_train_encoded, dtype=torch.long)  # Convert to torch.long\n",
    "\n",
    "val_inputs = torch.tensor(val_encodings['input_ids'])\n",
    "val_masks = torch.tensor(val_encodings['attention_mask'])\n",
    "val_labels = torch.tensor(y_val_encoded, dtype=torch.long)  # Convert to torch.long\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_predictions = []\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_predictions.extend(torch.argmax(outputs.logits, dim=1).cpu().detach().numpy())\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_predictions, val_true_labels = [], []\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "            val_predictions.extend(torch.argmax(outputs.logits, dim=1).cpu().detach().numpy())\n",
    "            val_true_labels.extend(inputs[\"labels\"].cpu().detach().numpy())\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    train_accuracy = accuracy_score(y_train_encoded, train_predictions)\n",
    "    val_accuracy = accuracy_score(y_val_encoded, val_predictions)\n",
    "\n",
    "    print(f\"BERT Classifier:\")\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(f\"  Training Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Classifier:\n",
      "Training Accuracy: 0.9117\n",
      "Testing Accuracy: 0.4086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub-0.23.0-py3.8.egg\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 77/77 [00:49<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 1:\n",
      "  Training Loss: 1.9885, Accuracy: 0.1128\n",
      "  Validation Loss: 1.8334, Accuracy: 0.3118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 77/77 [00:49<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 2:\n",
      "  Training Loss: 1.6987, Accuracy: 0.1292\n",
      "  Validation Loss: 1.6497, Accuracy: 0.4158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 77/77 [00:49<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 3:\n",
      "  Training Loss: 1.4401, Accuracy: 0.1186\n",
      "  Validation Loss: 1.4986, Accuracy: 0.4409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 77/77 [00:50<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 4:\n",
      "  Training Loss: 1.1081, Accuracy: 0.1398\n",
      "  Validation Loss: 1.4561, Accuracy: 0.4695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 77/77 [00:49<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 5:\n",
      "  Training Loss: 0.8210, Accuracy: 0.1357\n",
      "  Validation Loss: 1.5338, Accuracy: 0.4516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 77/77 [00:50<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 6:\n",
      "  Training Loss: 0.5599, Accuracy: 0.1243\n",
      "  Validation Loss: 1.5647, Accuracy: 0.5018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 77/77 [00:49<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 7:\n",
      "  Training Loss: 0.3722, Accuracy: 0.1186\n",
      "  Validation Loss: 1.7402, Accuracy: 0.4803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 77/77 [00:49<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 8:\n",
      "  Training Loss: 0.2504, Accuracy: 0.1038\n",
      "  Validation Loss: 1.8036, Accuracy: 0.4946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 77/77 [00:50<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 9:\n",
      "  Training Loss: 0.1728, Accuracy: 0.1267\n",
      "  Validation Loss: 1.8671, Accuracy: 0.5125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 77/77 [00:49<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier:\n",
      "Epoch 10:\n",
      "  Training Loss: 0.1490, Accuracy: 0.1243\n",
      "  Validation Loss: 1.8736, Accuracy: 0.5125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(\"propaganda_train.tsv\", delimiter='\\t', names=['label', 'sentence'])\n",
    "val_df = pd.read_csv(\"propaganda_val.tsv\", delimiter='\\t', names=['label', 'sentence'])\n",
    "\n",
    "# Define preprocessing functions\n",
    "def extract_text_between_bos_eos(text):\n",
    "    pattern = re.compile(r'<BOS>(.*?)<EOS>')\n",
    "    matches = pattern.findall(text)\n",
    "    return ' '.join(matches)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower().replace('<bos>', '').replace('<eos>', '')\n",
    "    tokens = word_tokenize(text)\n",
    "    remove_tokens = set(stopwords.words('english') + list(string.punctuation))\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in remove_tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['tagged_in_context'] = train_df['sentence'].apply(extract_text_between_bos_eos)\n",
    "val_df['tagged_in_context'] = val_df['sentence'].apply(extract_text_between_bos_eos)\n",
    "\n",
    "train_df['tagged_in_context'] = train_df['tagged_in_context'].apply(preprocess_text)\n",
    "val_df['tagged_in_context'] = val_df['tagged_in_context'].apply(preprocess_text)\n",
    "\n",
    "# Remove erroneous 'label' entries\n",
    "train_df = train_df[train_df['label'] != 'label']\n",
    "val_df = val_df[val_df['label'] != 'label']\n",
    "\n",
    "# Filter out 'not_propaganda' entries\n",
    "train_df = train_df[train_df['label'] != 'not_propaganda']\n",
    "val_df = val_df[val_df['label'] != 'not_propaganda']\n",
    "\n",
    "# Traditional Machine Learning with TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df['tagged_in_context'])\n",
    "X_val_tfidf = vectorizer.transform(val_df['tagged_in_context'])\n",
    "\n",
    "y_train = train_df['label'].tolist()\n",
    "y_val = val_df['label'].tolist()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "# Train Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "# Evaluate Multinomial Naive Bayes classifier\n",
    "nb_train_predictions = nb_classifier.predict(X_train_tfidf)\n",
    "nb_val_predictions = nb_classifier.predict(X_val_tfidf)\n",
    "\n",
    "nb_train_accuracy = accuracy_score(y_train_encoded, nb_train_predictions)\n",
    "nb_val_accuracy = accuracy_score(y_val_encoded, nb_val_predictions)\n",
    "\n",
    "print(\"MultinomialNB Classifier:\")\n",
    "print(f\"Training Accuracy: {nb_train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {nb_val_accuracy:.4f}\")\n",
    "\n",
    "# Fine-tune BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(train_df['label'].unique()))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "batch_size = 16\n",
    "train_encodings = tokenizer(list(train_df['tagged_in_context']), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(val_df['tagged_in_context']), truncation=True, padding=True)\n",
    "\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "train_labels = torch.tensor(y_train_encoded, dtype=torch.long)  # Convert to torch.long\n",
    "\n",
    "val_inputs = torch.tensor(val_encodings['input_ids'])\n",
    "val_masks = torch.tensor(val_encodings['attention_mask'])\n",
    "val_labels = torch.tensor(y_val_encoded, dtype=torch.long)  # Convert to torch.long\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_predictions = []\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_predictions.extend(torch.argmax(outputs.logits, dim=1).cpu().detach().numpy())\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_predictions, val_true_labels = [], []\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "            val_predictions.extend(torch.argmax(outputs.logits, dim=1).cpu().detach().numpy())\n",
    "            val_true_labels.extend(inputs[\"labels\"].cpu().detach().numpy())\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    train_accuracy = accuracy_score(y_train_encoded, train_predictions)\n",
    "    val_accuracy = accuracy_score(y_val_encoded, val_predictions)\n",
    "\n",
    "    print(f\"BERT Classifier:\")\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(f\"  Training Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
